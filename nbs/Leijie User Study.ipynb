{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classical-assets",
   "metadata": {},
   "source": [
    "# User Study Notebook \n",
    "-----------\n",
    "# EDA\n",
    "\n",
    "For this activity, you will be asked to explore a dataset using pandas. While you are exploring the dataset, a library called __lux__ will be activated that will suggest visualizations to you. The goal of this library is to track which functions you are using and suggest visualizations that plot the data from these functions. To see visualization recommendations simply execute the name of the pandas dataframe or series you would like to visualize and __lux__ will replace the default output with visualizations.\n",
    "\n",
    "Our goal is to have you explore the dataset how you normally would in python using __pandas__, and see how well __lux__ is able to recommend useful reccomendations.\n",
    "\n",
    "As you execute more pandas functions __lux__ will be able to reccomend more visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-coach",
   "metadata": {},
   "source": [
    "## Evaluation Prototype\n",
    "\n",
    "### Introduction\n",
    "For this activity we will be using a dataset about movies sales over different years with some info about the different movies. \n",
    "\n",
    "Imagine you are a machine learning engineer, and try to predict how the movie performs in terms of the worldwide gross from other attributes available in the dataset. To try to replicate a real-world analysis task, this dataset has not been thoroughly cleaned.\n",
    "\n",
    "We will set some exploratory questions during the whole process to guide you to understand more about the movies dataset. Please feel free to answer them in either figures or texts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hidden-guess",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T13:35:01.383506Z",
     "start_time": "2021-07-15T13:35:01.367288Z"
    }
   },
   "outputs": [],
   "source": [
    "import lux\n",
    "lux.logger = True\n",
    "import pandas as pd\n",
    "lux.config.default_display = \"lux\"\n",
    "\n",
    "# data load and setup\n",
    "df = pd.read_csv(\"../data/movies-sample.csv\")\n",
    "df['Release_Date'] = pd.to_datetime(df['Release_Date'], infer_datetime_format=True)\n",
    "df.history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-lodging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T08:23:25.634193Z",
     "start_time": "2021-07-15T08:23:25.629644Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.history) # should be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a0b03-c690-49f4-be75-75f71738b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-charlotte",
   "metadata": {},
   "source": [
    "### 1. Understand the dependent variable: Worldwide_Gross\n",
    "#### 1.1 Are there any null values in the `Worldwide_Gross` column? Try to clean them before proceeding\n",
    "> For now, there is no null value in this column, but ideally there should be. The user might benefit from our plots for the `dropna()` function calls. \n",
    "\n",
    "> Will - there are 5 nulls in worldwise gross, maybe we want more and so should change our data sample above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617667ae-05d0-44cb-8cd5-09fa714945f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 code v1\n",
    "df = df[~df.Worldwide_Gross.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848632be-fd90-4ff9-96bd-13855a5abf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc583d0-76db-4212-9eaf-e03436910f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 code v2\n",
    "df.dropna() # right now every single row has at least one value null so this returns an empty df I think... this is probably bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2e179-1b7c-4c1a-bb43-ee85cb7ac7de",
   "metadata": {},
   "source": [
    "#### 1.2 What is the distribution of the `Worldwide_Gross`? \n",
    "> Because of the recommendation (though for now, we do not log the column information when the column is specified as the `subset` parameter), the distribution of our dependent variable appears as the first one in the `Distribution` tab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88b619-35ea-4cd1-b5d0-f48626740ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 code\n",
    "df.Worldwide_Gross # this breaks for me?? throws a loc error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-tablet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T13:35:23.368380Z",
     "start_time": "2021-07-15T13:35:19.551984Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1.2 code\n",
    "df # worldwide gross shows up first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-liquid",
   "metadata": {},
   "source": [
    "### 2. Explore How `MPAA_Rating` predicts `Worldwide_Gross`\n",
    "After examining features in the dataset either simply from their names or from the figures we present in the `Distribution` or `Occurrence` tab, we are going to see how these features predict the `Worldwide_Gross` in turn and determine what features should be used when building our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7de7e9-9a76-4f66-9e68-78fc91b50697",
   "metadata": {},
   "source": [
    "#### 2.1 How many different `MPAA_Ratings` are there? What does the distribution look like?\n",
    "> Here our advantage is to present relevant information in a graph instead of placid numbers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-serbia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T08:23:31.767269Z",
     "start_time": "2021-07-15T08:23:31.618981Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"MPAA_Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c084c12-19f7-47be-9345-9a948e654bda",
   "metadata": {},
   "source": [
    "#### 2.2 How does the mean of `Worldwide_Gross` differ across different `MPAA_Rating`?\n",
    "> By exploring the `enhance` tab, the answer is readily available, though some experienced users may instinctively use `pd.groupby()` to solve this problem. In the later case, we could still present satisfactory figures in the `Column Groups` tab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-martial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T13:27:35.750886Z",
     "start_time": "2021-07-15T13:27:34.849593Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.corr() # why would they call corr for this question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-distribution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T08:23:39.210839Z",
     "start_time": "2021-07-15T08:23:37.331363Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(\"MPAA_Rating\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae697cc3-4625-4b06-b691-c2e0e23acc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"MPAA_Rating\").agg({\"MPAA_Rating\": \"mean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-classification",
   "metadata": {},
   "source": [
    "### 3. Feature selection: Decide between `IMDB_Rating` and `Rotten_Tomatoes_Rating`\n",
    " > In the second section, the user has already been familiar with how to explore a certain feature, so it should be safe to give them enough freedom to explore other features at this stage.\n",
    "\n",
    "From the first impression, `IMDB_Rating` and `Rotten_Tomatoes_Rating` are similar features, and we may only need to choose one of them as our predictor variable. Then the question is which one is more desirable? There are several dimensions that are worthy taking into consideration.\n",
    "#### 3.1 The number of non-null datapoints available\n",
    "\n",
    "#### 3.2 The general distribution of the feature\n",
    "> Actually, without writing more codes, users could find relevant figures in the recommendation tabs. \n",
    "\n",
    "#### 3.3 The correlation (the predictability) between each feature and the predicted variable\n",
    "> Are we going to override the function call `df[\"Worldwide_Gross\"].corr(df[\"IMDB_Rating\"])` to make it more convenient for users?\n",
    "\n",
    "#### 3.4 The relationship between `IMDB_Rating` and `Rotten_Tomatoes_Rating`: do they provide roughly the same information?\n",
    "> Here, after exploring these two attributes separately, the correlation graph between these two attributes appear the fist and we discover that these two features are generally highly correlated, so it will be more efficient to just use one in the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-kidney",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T08:51:29.903211Z",
     "start_time": "2021-07-15T08:51:25.734724Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf = df[pd.notna(df[\"IMDB_Rating\"])]\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-velvet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T08:24:00.564492Z",
     "start_time": "2021-07-15T08:23:57.269039Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf = df[pd.notna(df[\"Rotten_Tomatoes_Rating\"])]\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-vertex",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T07:23:07.321423Z",
     "start_time": "2021-07-15T07:23:07.315451Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df[\"Rotten_Tomatoes_Rating\"].corr(df[\"Worldwide_Gross\"]))\n",
    "print(df[\"IMDB_Rating\"].corr(df[\"Worldwide_Gross\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-willow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T08:21:47.865999Z",
     "start_time": "2021-07-15T08:21:47.858275Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-davis",
   "metadata": {},
   "source": [
    "### 4 Explore possible reasons for  the higher variance of the `Worldwide_Gross`\n",
    "From the correlation between the `Worldwide_Gross` and the `IMDB_Rating`, we find that generally speaking, with the `IMDB_Rating` increasing, the `Worldwide_Gross` for movies goes higher. However, for movies whose ratings fall into 6-8, the variance of the `Worldwide_Gross` is really high. Therefore, we plan to explore this subset of the original dataset, and understand what are the possible reasons.  \n",
    "> Although the real-world dataset is more complicated, what I have in mind in this section is that we modify the dataset so that we could explain the higher variance in this group by some nominal features, for example, the movie type. In this case, the `enhance` tab could somehow infer from the user's past attention to the `Worldwide_Gross` and the `IMDB_Rating` and draw a figure which in addition shows this nominal feature as the color channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-mistake",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T08:00:56.771046Z",
     "start_time": "2021-07-15T08:00:56.684463Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf = df[(df[\"IMDB_Rating\"] > 6) & (df[\"IMDB_Rating\"] < 8)]\n",
    "print(newdf[\"IMDB_Rating\"].corr(newdf[\"Worldwide_Gross\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-living",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T08:01:31.313931Z",
     "start_time": "2021-07-15T08:01:28.986525Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "history": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
