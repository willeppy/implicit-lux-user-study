{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Study Notebook \n",
    "-----------\n",
    "# EDA\n",
    "\n",
    "For this activity, you will be asked to explore a dataset using pandas. While you are exploring the dataset, a library called __lux__ will be activated that will suggest visualizations to you. The goal of this library is to track which functions you are using and suggest visualizations that plot the data from these functions. To see visualization recommendations simply execute the name of the pandas dataframe or series you would like to visualize and __lux__ will replace the default output with visualizations.\n",
    "\n",
    "Our goal is to have you explore the dataset how you normally would in python using __pandas__, and see how well __lux__ is able to recommend useful reccomendations.\n",
    "\n",
    "As you execute more pandas functions __lux__ will be able to reccomend more visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lux\n",
    "lux.logger = True\n",
    "import pandas as pd\n",
    "\n",
    "# from vega_datasets import data\n",
    "# lux.config.default_display = \"lux\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this activity we will be using a dataset about movies sales over different years with some info about the different movies. To try to replicate a real-world analysis task, this dataset has not been thoroughly cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/movies.csv\")\n",
    "df['Release_Date'] = pd.to_datetime(df['Release_Date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we modified the `Release_Date` column above, you can see that it is included in our recomendations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take a couple of minutes to familiarize yourself with this dataset. You will be asked some specific analysis questions later on but for now take some time to explore the features of the dataset, clean it or reformat it however you like, and become generally comfortable with what is in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# Clean and model\n",
    "Please clean the dataset and reformat as you would to prepare to run ML on this dataset. \n",
    "\n",
    "Then create a predictive model to predict high grossing movies (up to you how to define that). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, classifier_name, scoring=[\"accuracy\"]):\n",
    "    params = {}\n",
    "\n",
    "    if classifier_name == 'RF':\n",
    "        clf = RandomForestClassifier()\n",
    "        params = {\"n_estimators\": [10, 100], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "    if classifier_name == 'NN':\n",
    "        clf = MLPClassifier()\n",
    "        params = {\"hidden_layer_sizes\": [(100), (50, 25), (100, 50)], \"learning_rate_init\": [.01, .001], \"max_iter\": [100]}\n",
    "\n",
    " \n",
    "    if classifier_name == 'LR':\n",
    "        clf = LogisticRegression()\n",
    "        params = {\"penalty\": [\"l1\", \"l2\", \"elasticnet\"], \"solver\": [\"lbfgs\", \"saga\"], \"l1_ratio\":[.5], \"max_iter\":[200]}\n",
    "    \n",
    "    # evaluates with both scoring metrics but selects the best_model with the first\n",
    "    cv = GridSearchCV(clf, params, scoring=scoring, refit=scoring[0], cv=5)\n",
    "    cv.fit(X, y)\n",
    "    \n",
    "    # best estimator\n",
    "    best_clf = cv.best_estimator_\n",
    "    \n",
    "    # get average metrics\n",
    "    metric_df = pd.DataFrame(cv.cv_results_)\n",
    "    metrics = {}\n",
    "    \n",
    "    for m in scoring:\n",
    "        _val = metric_df[metric_df[f\"rank_test_{scoring[0]}\"] == 1][f\"mean_test_{m}\"].values[0]\n",
    "        metrics[m] = _val\n",
    "        \n",
    "    return best_clf, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Specific Tasks\n",
    "\n",
    "Great! Hopefully you feel comfortable with the general features of this dataset. You will now be asked a couple of specific questions about the data (that you may or may not have explored during your EDA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "How many different `MPAA_Rating`s are there? What does the distribution look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "What about Worldwide gross distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "What is the average Worldwide and US gross for different `MPAA_Rating` rated movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "Which does the distribution of median worldwide gross look like across genres? Which have the highest and lowest median worldwide gross?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "How do highly rated IMDB movies (>5) compare to non-highly rated (in general, answer however you please)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6\n",
    "How do highly rated `IMDB` and `Rotten tomatoes` ratings influence the other metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "# Part 3: Encoding feedback (optional)\n",
    "\n",
    "Please run the following cells and give feedback on how approiate the reccomended visual encoding is for that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.movies()\n",
    "df['Release_Date'] = pd.to_datetime(df['Release_Date'], infer_datetime_format=True)\n",
    "df.history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Value_Counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Distributor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.MPAA_Rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.MPAA_Rating == \"R\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.MPAA_Rating == \"R\") & (df.Worldwide_Gross > 85343400)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: df agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: df groupby agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"MPAA_Rating\").mean(\"Worldwide_Gross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Distributor\").agg({\"Worldwide_Gross\": \"mean\", \"US_Gross\": \"mean\", \"Runtime_min\": \"median\"})"
   ]
  }
 ],
 "metadata": {
  "history": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
